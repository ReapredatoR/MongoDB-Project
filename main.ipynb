{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB connection string\n",
    "connection_string = \"mongodb+srv://ReapredatoR:tkSXV9yiREji1Kcn@mongodbprojectmalinga.8viv4al.mongodb.net/?retryWrites=true&w=majority&appName=MongoDBprojectMalinga\"\n",
    "# Create a MongoClient object\n",
    "\n",
    "client = MongoClient(connection_string)\n",
    "db = client[\"Amazon_Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection\n",
    "customers_collection = db.customers\n",
    "customers_data = customers_collection.find()\n",
    "\n",
    "# Convert MongoDB data to a list of dictionaries\n",
    "data_list = list(customers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|Age|           Customer|Gender|_id|           addresses|      current_orders|        phone_number|recommended_products|\n",
      "+---+-------------------+------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| 51|     Gunner Ferrell|     M| C1|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 21|       Charity Dunn|     F| C7|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 4...|\n",
      "| 30|         Kylee Wang|     F| C9|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 4...|\n",
      "| 58|          Rowan Fox|     M|C13|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 33|     Brice Copeland|     M|C14|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 4...|\n",
      "| 43|      Yazmin Hughes|     F| C4|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 22|   Alyvia Mccormick|     F|C16|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 47|      Kelsey Brandt|     F| C8|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 28|       Ray Mcdaniel|     M| C5|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 64|       Lucian Cohen|     F|C12|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 50|         Ram Prasad|     M|C19|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 30|       Lillie Costa|     F| C2|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 66|     Calvin Carlson|     M|C15|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 38| Patience Valentine|     F|C17|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 4...|\n",
      "| 44|     Raelynn Dodson|     F| C3|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 19|   Jairo Fitzgerald|     M|C11|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 59|       Mark Mulldon|     M|C20|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 70|     Aubree Holland|     M|C18|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 45|         Billy Ross|     M|C10|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 64|Jamarcus Montgomery|     M| C6|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "+---+-------------------+------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Convert the list of dictionaries to DataFrame\n",
    "df = spark.createDataFrame(data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+------+----+\n",
      "|summary|               Age|        Customer|Gender| _id|\n",
      "+-------+------------------+----------------+------+----+\n",
      "|  count|                20|              20|    20|  20|\n",
      "|   mean|              44.1|            NULL|  NULL|NULL|\n",
      "| stddev|16.091514599479538|            NULL|  NULL|NULL|\n",
      "|    min|                19|Alyvia Mccormick|     F|  C1|\n",
      "|    max|                70|   Yazmin Hughes|     M|  C9|\n",
      "+-------+------------------+----------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics using describe()\n",
    "summary_stats = df.describe()\n",
    "\n",
    "# Display summary statistics\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection\n",
    "ratings_collection = db.ratings\n",
    "ratings_data = ratings_collection.find()\n",
    "\n",
    "# Convert MongoDB data to a list of dictionaries\n",
    "ratings_data_list = list(ratings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------------+-----------------+----------+-------------------+------+\n",
      "|  _id|customer_id|         order_date|         order_id|product_id|     published_date|rating|\n",
      "+-----+-----------+-------------------+-----------------+----------+-------------------+------+\n",
      "|  RA1|         C6|2022-10-01 22:44:11| 20221001224411C6|      FP13|2022-10-02 22:44:11|     4|\n",
      "| RA12|        C13|2022-10-02 09:17:17|20221002091717C13|        B4|2022-10-09 09:17:17|     4|\n",
      "| RA14|         C9|2022-10-02 15:34:55| 20221002153455C9|       CD3|2022-10-09 15:34:55|     2|\n",
      "| RA15|        C17|2022-10-02 16:16:55|20221002161655C17|        B8|2022-10-09 16:16:55|     2|\n",
      "| RA18|        C12|2022-10-03 19:51:00|20221003195100C12|       HA4|2022-10-10 19:51:00|     3|\n",
      "| RA27|        C19|2022-10-05 20:48:12|20221005204812C19|       CD6|2022-10-12 20:48:12|     4|\n",
      "| RA30|        C15|2022-10-06 19:21:29|20221006192129C15|       CD5|2022-10-13 19:21:29|     5|\n",
      "| RA33|         C3|2022-10-07 08:36:31| 20221007083631C3|       CD7|2022-10-14 08:36:31|     2|\n",
      "| RA34|         C3|2022-10-13 09:04:00| 20221013090400C3|      FP13|2022-10-14 09:04:00|     5|\n",
      "| RA40|         C6|2022-10-15 03:01:09| 20221015030109C6|       FP4|2022-10-16 03:01:09|     1|\n",
      "| RA53|        C20|2022-10-12 05:53:41|20221012055341C20|       PH2|2022-10-19 05:53:41|     5|\n",
      "| RA59|         C8|2022-10-13 05:29:26| 20221013052926C8|       PH9|2022-10-20 05:29:26|     5|\n",
      "| RA76|        C10|2022-10-17 04:27:37|20221017042737C10|       HA7|2022-10-24 04:27:37|     1|\n",
      "| RA77|         C4|2022-10-17 09:15:07| 20221017091507C4|        B5|2022-10-24 09:15:07|     5|\n",
      "| RA81|         C8|2022-10-18 10:20:42| 20221018102042C8|       CD6|2022-10-25 10:20:42|     4|\n",
      "| RA86|        C16|2022-10-19 21:10:01|20221019211001C16|       CD9|2022-10-26 21:10:01|     4|\n",
      "| RA96|         C7|2022-10-22 01:00:10| 20221022010010C7|       HA3|2022-10-29 01:00:10|     4|\n",
      "| RA97|         C9|2022-10-22 06:31:20| 20221022063120C9|        B2|2022-10-29 06:31:20|     4|\n",
      "|RA106|         C2|2022-10-30 02:46:50| 20221030024650C2|      FP11|2022-10-31 02:46:50|     2|\n",
      "| RA13|         C6|2022-10-02 14:37:29| 20221002143729C6|       PH4|2022-10-09 14:37:29|     4|\n",
      "+-----+-----------+-------------------+-----------------+----------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to DataFrame\n",
    "df2 = spark.createDataFrame(ratings_data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rating: 2.9854545454545454\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the average rating of all products\n",
    "avg_rating = df2.select(avg(\"rating\")).collect()[0][0]\n",
    "print(\"Average Rating:\", avg_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|product_id|avg_rating|\n",
      "+----------+----------+\n",
      "|        B2|       4.2|\n",
      "|      CD10|       4.0|\n",
      "|       HA3|       4.0|\n",
      "|       CD2|       4.0|\n",
      "|        B9|       3.8|\n",
      "|      FP10|       3.8|\n",
      "|       HA2|       3.8|\n",
      "|       FP8|       3.8|\n",
      "|      HA10|       3.8|\n",
      "|       FP1|       3.6|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify popular products (top 10 products by rating)\n",
    "top_10_products = df2.groupBy(\"product_id\").agg(avg(\"rating\").alias(\"avg_rating\")).orderBy(\"avg_rating\", ascending=False).limit(10)\n",
    "top_10_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ['B2', 'CD10', 'HA3', 'CD2', 'B9', 'FP10', 'HA2', 'FP8', 'HA10', 'FP1']\n",
    "products_collection = db.products\n",
    "products_data = products_collection.find({'_id': {'$in': ids}}, {'name': 1, '_id': 1, 'category': 1})\n",
    "\n",
    "products_data_list = list(products_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------------------+\n",
      "| _id|category|                name|\n",
      "+----+--------+--------------------+\n",
      "|  B2|       B|          Cold Grave|\n",
      "|  B9|       B|Thinking, Fast an...|\n",
      "|CD10|      CD|    Hotel California|\n",
      "| CD2|      CD|     Led Zepellin IV|\n",
      "| FP1|   fresh|                Tart|\n",
      "|FP10|   fresh|             Whiskey|\n",
      "| FP8|   fresh|                Beer|\n",
      "|HA10|      HA|LED Starry Sky Pr...|\n",
      "| HA2|      HA|Eufy by Anker, Bo...|\n",
      "| HA3|      HA|EasyAcc Coffee Mu...|\n",
      "+----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to DataFrame\n",
    "df3 = spark.createDataFrame(products_data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection\n",
    "suppliers_collection = db.suppliers\n",
    "suppliers_data = suppliers_collection.find()\n",
    "\n",
    "# Convert MongoDB data to a list of dictionaries\n",
    "ratings_data_list = list(suppliers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+--------------------+--------------------+---------+--------------------+\n",
      "|_id|             address|      city|            location|                name|post_code|  realtime_inventory|\n",
      "+---+--------------------+----------+--------------------+--------------------+---------+--------------------+\n",
      "| W1|Manchester Airpor...|Manchester|{coordinates -> [...|      Amazon UK MAN1|  M90 5DL|[{quantity -> 14,...|\n",
      "|ST1|    19-25 Piccadilly|Manchester|{coordinates -> [...|Morrisons Piccadilly|   M1 1LU|[{quantity -> 1, ...|\n",
      "| W2|Amazon UK Service...|Warrington|{coordinates -> [...|      Amazon UK MAN2|  WA5 3XA|[{quantity -> 9, ...|\n",
      "| W3|DHL Preston Brook...|   Runcorn|{coordinates -> [...|      Amazom UK XUKA|  WA7 3BN|[{quantity -> 8, ...|\n",
      "|ST4|         22 Ducie St|Manchester|{coordinates -> [...|     Morrisons Ducie|   M1 2DP|[{quantity -> 25,...|\n",
      "|ST5|Unit 4, Moho Buil...|Manchester|{coordinates -> [...|      Morrisons Boho|  M15 4JY|[{quantity -> 11,...|\n",
      "|ST3|333 Stretford Rd,...|Manchester|{coordinates -> [...| Morrisons Stretford|  M15 4AY|[{quantity -> 5, ...|\n",
      "|ST6|418B Wilbraham Rd...|Manchester|{coordinates -> [...| Morrisons Wilbraham|  M21 0UA|[{quantity -> 25,...|\n",
      "|ST2|Off Grafton St, O...|Manchester|{coordinates -> [...|    Morrisons Oxford|  M13 9NU|[{quantity -> 2, ...|\n",
      "+---+--------------------+----------+--------------------+--------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to DataFrame\n",
    "df4 = spark.createDataFrame(ratings_data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      city|count|\n",
      "+----------+-----+\n",
      "|Manchester|    7|\n",
      "|Warrington|    1|\n",
      "|   Runcorn|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by city and count the number of suppliers for each city\n",
    "city_counts = df4.groupBy('city').count()\n",
    "\n",
    "# Sort the counts in descending order\n",
    "city_counts_sorted = city_counts.orderBy(F.desc('count'))\n",
    "\n",
    "# Show the city name and count in descending order\n",
    "city_counts_sorted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of suppliers: 9\n"
     ]
    }
   ],
   "source": [
    "# Count the number of suppliers\n",
    "suppliers_count = df4.count()\n",
    "\n",
    "# Print the count of suppliers\n",
    "print(\"Number of suppliers:\", suppliers_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection and select only necessary columns\n",
    "partners_collection = db.partners\n",
    "partners_data = partners_collection.find({}, {'name': 1, 'deliveries_made': 1, 'number_of_week': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MongoDB data to a list of dictionaries\n",
    "partners_data_list = list(partners_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to DataFrame\n",
    "df_partners = spark.createDataFrame(partners_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+--------------+\n",
      "|_id|deliveries_made|           name|number_of_week|\n",
      "+---+---------------+---------------+--------------+\n",
      "|PA3|            140|  Hashim Ridwan|             5|\n",
      "|PA1|            150|      Mike Dean|            12|\n",
      "|PA2|             90|Robert Chaniago|             9|\n",
      "|PA4|            125| Sebastian Kanu|            10|\n",
      "|PA5|            120|     Alan Smith|            12|\n",
      "+---+---------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "df_partners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+--------------+-----------------------+\n",
      "|_id|deliveries_made|           name|number_of_week|avg_deliveries_per_week|\n",
      "+---+---------------+---------------+--------------+-----------------------+\n",
      "|PA3|            140|  Hashim Ridwan|             5|                   28.0|\n",
      "|PA1|            150|      Mike Dean|            12|                   12.5|\n",
      "|PA2|             90|Robert Chaniago|             9|                   10.0|\n",
      "|PA4|            125| Sebastian Kanu|            10|                   12.5|\n",
      "|PA5|            120|     Alan Smith|            12|                   10.0|\n",
      "+---+---------------+---------------+--------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average deliveries made per week\n",
    "df_partners = df_partners.withColumn('avg_deliveries_per_week', df_partners['deliveries_made'] / df_partners['number_of_week'])\n",
    "\n",
    "df_partners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the partner with the highest average deliveries per week\n",
    "max_avg_deliveries_per_week = df_partners.groupBy().agg({'avg_deliveries_per_week': 'max'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to find the partner(s) with the highest average deliveries per week\n",
    "partners_with_max_avg_deliveries = df_partners.filter(df_partners['avg_deliveries_per_week'] == max_avg_deliveries_per_week).select('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         name|\n",
      "+-------------+\n",
      "|Hashim Ridwan|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "partners_with_max_avg_deliveries.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
