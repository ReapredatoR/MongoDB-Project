{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB connection string\n",
    "connection_string = \"mongodb+srv://ReapredatoR:tkSXV9yiREji1Kcn@mongodbprojectmalinga.8viv4al.mongodb.net/?retryWrites=true&w=majority&appName=MongoDBprojectMalinga\"\n",
    "# Create a MongoClient object\n",
    "\n",
    "client = MongoClient(connection_string)\n",
    "db = client[\"Amazon_Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection\n",
    "customers_collection = db.customers\n",
    "customers_data = customers_collection.find()\n",
    "\n",
    "# Convert MongoDB data to a list of dictionaries\n",
    "data_list = list(customers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 20:36:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|Age|           Customer|Gender|_id|           addresses|      current_orders|        phone_number|recommended_products|\n",
      "+---+-------------------+------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| 51|     Gunner Ferrell|     M| C1|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 30|       Lillie Costa|     F| C2|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 44|     Raelynn Dodson|     F| C3|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 43|      Yazmin Hughes|     F| C4|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 28|       Ray Mcdaniel|     M| C5|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 64|Jamarcus Montgomery|     M| C6|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 21|       Charity Dunn|     F| C7|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 4...|\n",
      "| 47|      Kelsey Brandt|     F| C8|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 30|         Kylee Wang|     F| C9|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 4...|\n",
      "| 45|         Billy Ross|     M|C10|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 19|   Jairo Fitzgerald|     M|C11|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 64|       Lucian Cohen|     F|C12|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 58|          Rowan Fox|     M|C13|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 33|     Brice Copeland|     M|C14|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 4...|\n",
      "| 66|     Calvin Carlson|     M|C15|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 22|   Alyvia Mccormick|     F|C16|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 38| Patience Valentine|     F|C17|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 4...|\n",
      "| 70|     Aubree Holland|     M|C18|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 50|         Ram Prasad|     M|C19|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "| 59|       Mark Mulldon|     M|C20|[{location -> {co...|[{shipping_id -> ...|{$numberLong -> 4...|[{avg_rating -> 3...|\n",
      "+---+-------------------+------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Convert the list of dictionaries to DataFrame\n",
    "df = spark.createDataFrame(data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+------+----+\n",
      "|summary|               Age|        Customer|Gender| _id|\n",
      "+-------+------------------+----------------+------+----+\n",
      "|  count|                20|              20|    20|  20|\n",
      "|   mean|              44.1|            NULL|  NULL|NULL|\n",
      "| stddev|16.091514599479538|            NULL|  NULL|NULL|\n",
      "|    min|                19|Alyvia Mccormick|     F|  C1|\n",
      "|    max|                70|   Yazmin Hughes|     M|  C9|\n",
      "+-------+------------------+----------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics using describe()\n",
    "summary_stats = df.describe()\n",
    "\n",
    "# Display summary statistics\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection\n",
    "ratings_collection = db.ratings\n",
    "ratings_data = ratings_collection.find()\n",
    "\n",
    "# Convert MongoDB data to a list of dictionaries\n",
    "ratings_data_list = list(ratings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+-----------------+----------+-------------------+------+\n",
      "| _id|customer_id|         order_date|         order_id|product_id|     published_date|rating|\n",
      "+----+-----------+-------------------+-----------------+----------+-------------------+------+\n",
      "| RA1|         C6|2022-10-01 22:44:11| 20221001224411C6|      FP13|2022-10-02 22:44:11|     4|\n",
      "| RA2|        C20|2022-10-04 16:52:33|20221004165233C20|       FP2|2022-10-05 16:52:33|     1|\n",
      "| RA3|         C5|2022-10-04 20:32:04| 20221004203204C5|      FP12|2022-10-05 20:32:04|     1|\n",
      "| RA4|        C17|2022-10-05 03:13:11|20221005031311C17|       FP1|2022-10-06 03:13:11|     1|\n",
      "| RA5|        C12|2022-10-06 19:37:37|20221006193737C12|       FP3|2022-10-07 19:37:37|     1|\n",
      "| RA6|         C4|2022-10-01 05:09:39| 20221001050939C4|        B3|2022-10-08 05:09:39|     1|\n",
      "| RA7|        C19|2022-10-01 10:27:13|20221001102713C19|       CD2|2022-10-08 10:27:13|     4|\n",
      "| RA8|        C16|2022-10-01 21:40:36|20221001214036C16|       HA6|2022-10-08 21:40:36|     1|\n",
      "| RA9|         C7|2022-10-01 22:19:01| 20221001221901C7|       CD8|2022-10-08 22:19:01|     1|\n",
      "|RA10|         C4|2022-10-02 01:03:27| 20221002010327C4|        B5|2022-10-09 01:03:27|     2|\n",
      "|RA11|         C2|2022-10-02 03:41:49| 20221002034149C2|       CD8|2022-10-09 03:41:49|     3|\n",
      "|RA12|        C13|2022-10-02 09:17:17|20221002091717C13|        B4|2022-10-09 09:17:17|     4|\n",
      "|RA13|         C6|2022-10-02 14:37:29| 20221002143729C6|       PH4|2022-10-09 14:37:29|     4|\n",
      "|RA14|         C9|2022-10-02 15:34:55| 20221002153455C9|       CD3|2022-10-09 15:34:55|     2|\n",
      "|RA15|        C17|2022-10-02 16:16:55|20221002161655C17|        B8|2022-10-09 16:16:55|     2|\n",
      "|RA16|        C14|2022-10-02 16:51:22|20221002165122C14|        B7|2022-10-09 16:51:22|     1|\n",
      "|RA17|        C19|2022-10-09 00:09:19|20221009000919C19|       FP9|2022-10-10 00:09:19|     1|\n",
      "|RA18|        C12|2022-10-03 19:51:00|20221003195100C12|       HA4|2022-10-10 19:51:00|     3|\n",
      "|RA19|        C20|2022-10-09 20:25:21|20221009202521C20|       FP4|2022-10-10 20:25:21|     4|\n",
      "|RA20|        C14|2022-10-10 01:29:46|20221010012946C14|       FP1|2022-10-11 01:29:46|     5|\n",
      "+----+-----------+-------------------+-----------------+----------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to DataFrame\n",
    "df2 = spark.createDataFrame(ratings_data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rating: 2.9854545454545454\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the average rating of all products\n",
    "avg_rating = df2.select(avg(\"rating\")).collect()[0][0]\n",
    "print(\"Average Rating:\", avg_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|product_id|avg_rating|\n",
      "+----------+----------+\n",
      "|        B2|       4.2|\n",
      "|       CD2|       4.0|\n",
      "|       HA3|       4.0|\n",
      "|      CD10|       4.0|\n",
      "|      FP10|       3.8|\n",
      "|       HA2|       3.8|\n",
      "|        B9|       3.8|\n",
      "|      HA10|       3.8|\n",
      "|       FP8|       3.8|\n",
      "|       PH4|       3.6|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify popular products (top 10 products by rating)\n",
    "top_10_products = df2.groupBy(\"product_id\").agg(avg(\"rating\").alias(\"avg_rating\")).orderBy(\"avg_rating\", ascending=False).limit(10)\n",
    "top_10_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ['B2', 'CD10', 'HA3', 'CD2', 'B9', 'FP10', 'HA2', 'FP8', 'HA10', 'FP1']\n",
    "products_collection = db.products\n",
    "products_data = products_collection.find({'_id': {'$in': ids}}, {'name': 1, '_id': 1, 'category': 1})\n",
    "\n",
    "products_data_list = list(products_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------------------+\n",
      "| _id|category|                name|\n",
      "+----+--------+--------------------+\n",
      "|  B2|       B|          Cold Grave|\n",
      "|  B9|       B|Thinking, Fast an...|\n",
      "|CD10|      CD|    Hotel California|\n",
      "| CD2|      CD|     Led Zepellin IV|\n",
      "| FP1|   fresh|                Tart|\n",
      "|FP10|   fresh|             Whiskey|\n",
      "| FP8|   fresh|                Beer|\n",
      "|HA10|      HA|LED Starry Sky Pr...|\n",
      "| HA2|      HA|Eufy by Anker, Bo...|\n",
      "| HA3|      HA|EasyAcc Coffee Mu...|\n",
      "+----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to DataFrame\n",
    "df3 = spark.createDataFrame(products_data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection\n",
    "suppliers_collection = db.suppliers\n",
    "suppliers_data = suppliers_collection.find()\n",
    "\n",
    "# Convert MongoDB data to a list of dictionaries\n",
    "ratings_data_list = list(suppliers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+--------------------+--------------------+---------+--------------------+\n",
      "|_id|             address|      city|            location|                name|post_code|  realtime_inventory|\n",
      "+---+--------------------+----------+--------------------+--------------------+---------+--------------------+\n",
      "| W1|Manchester Airpor...|Manchester|{coordinates -> [...|      Amazon UK MAN1|  M90 5DL|[{quantity -> 14,...|\n",
      "| W2|Amazon UK Service...|Warrington|{coordinates -> [...|      Amazon UK MAN2|  WA5 3XA|[{quantity -> 9, ...|\n",
      "| W3|DHL Preston Brook...|   Runcorn|{coordinates -> [...|      Amazom UK XUKA|  WA7 3BN|[{quantity -> 8, ...|\n",
      "|ST1|    19-25 Piccadilly|Manchester|{coordinates -> [...|Morrisons Piccadilly|   M1 1LU|[{quantity -> 1, ...|\n",
      "|ST2|Off Grafton St, O...|Manchester|{coordinates -> [...|    Morrisons Oxford|  M13 9NU|[{quantity -> 2, ...|\n",
      "|ST3|333 Stretford Rd,...|Manchester|{coordinates -> [...| Morrisons Stretford|  M15 4AY|[{quantity -> 5, ...|\n",
      "|ST4|         22 Ducie St|Manchester|{coordinates -> [...|     Morrisons Ducie|   M1 2DP|[{quantity -> 25,...|\n",
      "|ST5|Unit 4, Moho Buil...|Manchester|{coordinates -> [...|      Morrisons Boho|  M15 4JY|[{quantity -> 11,...|\n",
      "|ST6|418B Wilbraham Rd...|Manchester|{coordinates -> [...| Morrisons Wilbraham|  M21 0UA|[{quantity -> 25,...|\n",
      "+---+--------------------+----------+--------------------+--------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to DataFrame\n",
    "df4 = spark.createDataFrame(ratings_data_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      city|count|\n",
      "+----------+-----+\n",
      "|Manchester|    7|\n",
      "|Warrington|    1|\n",
      "|   Runcorn|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by city and count the number of suppliers for each city\n",
    "city_counts = df4.groupBy('city').count()\n",
    "\n",
    "# Sort the counts in descending order\n",
    "city_counts_sorted = city_counts.orderBy(F.desc('count'))\n",
    "\n",
    "# Show the city name and count in descending order\n",
    "city_counts_sorted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of suppliers: 9\n"
     ]
    }
   ],
   "source": [
    "# Count the number of suppliers\n",
    "suppliers_count = df4.count()\n",
    "\n",
    "# Print the count of suppliers\n",
    "print(\"Number of suppliers:\", suppliers_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection and select only necessary columns\n",
    "partners_collection = db.partners\n",
    "partners_data = partners_collection.find({}, {'name': 1, 'deliveries_made': 1, 'number_of_week': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MongoDB data to a list of dictionaries\n",
    "partners_data_list = list(partners_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to DataFrame\n",
    "df_partners = spark.createDataFrame(partners_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+--------------+\n",
      "|_id|deliveries_made|           name|number_of_week|\n",
      "+---+---------------+---------------+--------------+\n",
      "|PA1|            150|      Mike Dean|            12|\n",
      "|PA2|             90|Robert Chaniago|             9|\n",
      "|PA3|            140|  Hashim Ridwan|             5|\n",
      "|PA4|            125| Sebastian Kanu|            10|\n",
      "|PA5|            120|     Alan Smith|            12|\n",
      "+---+---------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "df_partners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+--------------+-----------------------+\n",
      "|_id|deliveries_made|           name|number_of_week|avg_deliveries_per_week|\n",
      "+---+---------------+---------------+--------------+-----------------------+\n",
      "|PA1|            150|      Mike Dean|            12|                   12.5|\n",
      "|PA2|             90|Robert Chaniago|             9|                   10.0|\n",
      "|PA3|            140|  Hashim Ridwan|             5|                   28.0|\n",
      "|PA4|            125| Sebastian Kanu|            10|                   12.5|\n",
      "|PA5|            120|     Alan Smith|            12|                   10.0|\n",
      "+---+---------------+---------------+--------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average deliveries made per week\n",
    "df_partners = df_partners.withColumn('avg_deliveries_per_week', df_partners['deliveries_made'] / df_partners['number_of_week'])\n",
    "\n",
    "df_partners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the partner with the highest average deliveries per week\n",
    "max_avg_deliveries_per_week = df_partners.groupBy().agg({'avg_deliveries_per_week': 'max'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to find the partner(s) with the highest average deliveries per week\n",
    "partners_with_max_avg_deliveries = df_partners.filter(df_partners['avg_deliveries_per_week'] == max_avg_deliveries_per_week).select('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         name|\n",
      "+-------------+\n",
      "|Hashim Ridwan|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "partners_with_max_avg_deliveries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB collection\n",
    "past_orders_collection = db.pastOrders\n",
    "past_orders_data = past_orders_collection.find()\n",
    "\n",
    "# Convert MongoDB data to a list of dictionaries\n",
    "past_orders_data_list = list(past_orders_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----------+--------------------+----------+----------+-----------+-----------+------------+\n",
      "|              _id|         order_date|customer_id|       order_details|total_cost|partner_id|shipping_id|supplier_id|order_status|\n",
      "+-----------------+-------------------+-----------+--------------------+----------+----------+-----------+-----------+------------+\n",
      "| 20221001050939C4|2022-10-01 05:09:00|         C4|[{cost -> 3.2, qu...|       6.4|       PA1|        AD4|         W2|           5|\n",
      "|20221001102713C19|2022-10-01 10:27:00|        C19|[{cost -> 16.99, ...|     50.97|       PA4|       AD21|         W2|           5|\n",
      "|20221001214036C16|2022-10-01 21:40:00|        C16|[{cost -> 200, qu...|    1000.0|       PA3|       AD17|         W3|           5|\n",
      "| 20221001221901C7|2022-10-01 22:19:00|         C7|[{cost -> 329.39,...|   1317.56|       PA1|        AD7|         W1|           5|\n",
      "| 20221001224411C6|2022-10-01 22:44:00|         C6|[{cost -> 849, qu...|     849.0|       PA5|        AD6|        ST3|           5|\n",
      "| 20221002010327C4|2022-10-02 01:03:00|         C4|[{cost -> 4, quan...|      20.0|       PA5|        AD4|         W2|           5|\n",
      "| 20221002034149C2|2022-10-02 03:41:00|         C2|[{cost -> 2.8, qu...|       5.6|       PA3|        AD2|         W1|           5|\n",
      "|20221002091717C13|2022-10-02 09:17:00|        C13|[{cost -> 21.97, ...|     43.94|       PA2|       AD13|         W2|           5|\n",
      "| 20221002143729C6|2022-10-02 14:37:00|         C6|[{cost -> 37.04, ...|    111.12|       PA2|        AD6|         W2|           5|\n",
      "| 20221002153455C9|2022-10-02 15:34:00|         C9|[{cost -> 3, quan...|      15.0|       PA4|        AD9|         W2|           5|\n",
      "|20221002161655C17|2022-10-02 16:16:00|        C17|[{cost -> 190.85,...|    190.85|       PA4|       AD18|         W2|           5|\n",
      "|20221002165122C14|2022-10-02 16:51:00|        C14|[{cost -> 16.58, ...|     49.74|       PA3|       AD15|         W1|           5|\n",
      "|20221003195100C12|2022-10-03 19:51:00|        C12|[{cost -> 13.5, q...|      40.5|       PA2|       AD12|         W2|           5|\n",
      "|20221004092039C11|2022-10-04 09:20:00|        C11|[{cost -> 58.07, ...|    232.28|       PA5|       AD11|         W1|           5|\n",
      "|20221004100456C17|2022-10-04 10:04:00|        C17|[{cost -> 13.5, q...|      40.5|       PA4|       AD18|         W2|           5|\n",
      "|20221004152651C12|2022-10-04 15:26:00|        C12|[{cost -> 9.99, q...|     29.97|       PA4|       AD12|         W2|           5|\n",
      "|20221004165233C20|2022-10-04 16:52:00|        C20|[{cost -> 849, qu...|    2547.0|       PA2|       AD22|        ST1|           5|\n",
      "| 20221004203204C5|2022-10-04 20:32:00|         C5|[{cost -> 2, quan...|       4.0|       PA3|        AD5|        ST5|           5|\n",
      "|20221005031311C17|2022-10-05 03:13:00|        C17|[{cost -> 769, qu...|    3076.0|       PA2|       AD18|        ST2|           5|\n",
      "|20221005204812C19|2022-10-05 20:48:00|        C19|[{cost -> 16.58, ...|      82.9|       PA2|       AD21|         W3|           5|\n",
      "+-----------------+-------------------+-----------+--------------------+----------+----------+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 20:36:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PastOrdersAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Convert the list of dictionaries to DataFrame\n",
    "df6 = spark.createDataFrame(pd.DataFrame(past_orders_data_list))\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df6.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, month, quarter\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PastOrdersAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Fetch data from MongoDB collection\n",
    "past_orders_collection = db.pastOrders\n",
    "past_orders_data = past_orders_collection.find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----------+--------------------+----------+----------+-----------+-----------+------------+\n",
      "|              _id|         order_date|customer_id|       order_details|total_cost|partner_id|shipping_id|supplier_id|order_status|\n",
      "+-----------------+-------------------+-----------+--------------------+----------+----------+-----------+-----------+------------+\n",
      "| 20221001050939C4|2022-10-01 05:09:00|         C4|[{cost -> 3.2, qu...|       6.4|       PA1|        AD4|         W2|           5|\n",
      "|20221001102713C19|2022-10-01 10:27:00|        C19|[{cost -> 16.99, ...|     50.97|       PA4|       AD21|         W2|           5|\n",
      "|20221001214036C16|2022-10-01 21:40:00|        C16|[{cost -> 200, qu...|    1000.0|       PA3|       AD17|         W3|           5|\n",
      "| 20221001221901C7|2022-10-01 22:19:00|         C7|[{cost -> 329.39,...|   1317.56|       PA1|        AD7|         W1|           5|\n",
      "| 20221001224411C6|2022-10-01 22:44:00|         C6|[{cost -> 849, qu...|     849.0|       PA5|        AD6|        ST3|           5|\n",
      "| 20221002010327C4|2022-10-02 01:03:00|         C4|[{cost -> 4, quan...|      20.0|       PA5|        AD4|         W2|           5|\n",
      "| 20221002034149C2|2022-10-02 03:41:00|         C2|[{cost -> 2.8, qu...|       5.6|       PA3|        AD2|         W1|           5|\n",
      "|20221002091717C13|2022-10-02 09:17:00|        C13|[{cost -> 21.97, ...|     43.94|       PA2|       AD13|         W2|           5|\n",
      "| 20221002143729C6|2022-10-02 14:37:00|         C6|[{cost -> 37.04, ...|    111.12|       PA2|        AD6|         W2|           5|\n",
      "| 20221002153455C9|2022-10-02 15:34:00|         C9|[{cost -> 3, quan...|      15.0|       PA4|        AD9|         W2|           5|\n",
      "|20221002161655C17|2022-10-02 16:16:00|        C17|[{cost -> 190.85,...|    190.85|       PA4|       AD18|         W2|           5|\n",
      "|20221002165122C14|2022-10-02 16:51:00|        C14|[{cost -> 16.58, ...|     49.74|       PA3|       AD15|         W1|           5|\n",
      "|20221003195100C12|2022-10-03 19:51:00|        C12|[{cost -> 13.5, q...|      40.5|       PA2|       AD12|         W2|           5|\n",
      "|20221004092039C11|2022-10-04 09:20:00|        C11|[{cost -> 58.07, ...|    232.28|       PA5|       AD11|         W1|           5|\n",
      "|20221004100456C17|2022-10-04 10:04:00|        C17|[{cost -> 13.5, q...|      40.5|       PA4|       AD18|         W2|           5|\n",
      "|20221004152651C12|2022-10-04 15:26:00|        C12|[{cost -> 9.99, q...|     29.97|       PA4|       AD12|         W2|           5|\n",
      "|20221004165233C20|2022-10-04 16:52:00|        C20|[{cost -> 849, qu...|    2547.0|       PA2|       AD22|        ST1|           5|\n",
      "| 20221004203204C5|2022-10-04 20:32:00|         C5|[{cost -> 2, quan...|       4.0|       PA3|        AD5|        ST5|           5|\n",
      "|20221005031311C17|2022-10-05 03:13:00|        C17|[{cost -> 769, qu...|    3076.0|       PA2|       AD18|        ST2|           5|\n",
      "|20221005204812C19|2022-10-05 20:48:00|        C19|[{cost -> 16.58, ...|      82.9|       PA2|       AD21|         W3|           5|\n",
      "+-----------------+-------------------+-----------+--------------------+----------+----------+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert MongoDB data to a list of dictionaries\n",
    "past_orders_data_list = list(past_orders_data)\n",
    "\n",
    "# Convert the list of dictionaries to DataFrame\n",
    "df = spark.createDataFrame(pd.DataFrame(past_orders_data_list))\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert order_date to date format\n",
    "df = df.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "\n",
    "# Extract month and quarter from order_date\n",
    "df = df.withColumn(\"month\", month(col(\"order_date\")))\n",
    "df = df.withColumn(\"quarter\", quarter(col(\"order_date\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data based on month and quarter, and calculate total sales\n",
    "monthly_sales = df.groupBy(\"month\").sum(\"total_cost\").orderBy(\"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly sales:\n",
      "+-----+------------------+\n",
      "|month|   sum(total_cost)|\n",
      "+-----+------------------+\n",
      "|   10|38158.579999999994|\n",
      "|   11|          53767.09|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "print(\"Monthly sales:\")\n",
    "monthly_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_sales = df.groupBy(\"quarter\").sum(\"total_cost\").orderBy(\"quarter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly sales:\n",
      "+-------+---------------+\n",
      "|quarter|sum(total_cost)|\n",
      "+-------+---------------+\n",
      "|      4|       91925.67|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Quarterly sales:\")\n",
    "quarterly_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly average sales:\n",
      "+-----+------------------+\n",
      "|month|   avg(total_cost)|\n",
      "+-----+------------------+\n",
      "|   10|282.65614814814813|\n",
      "|   11| 384.0506428571428|\n",
      "+-----+------------------+\n",
      "\n",
      "Quarterly average sales:\n",
      "+-------+------------------+\n",
      "|quarter|   avg(total_cost)|\n",
      "+-------+------------------+\n",
      "|      4|334.27516363636363|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average sales per month and quarter\n",
    "monthly_avg_sales = df.groupBy(\"month\").avg(\"total_cost\").orderBy(\"month\")\n",
    "quarterly_avg_sales = df.groupBy(\"quarter\").avg(\"total_cost\").orderBy(\"quarter\")\n",
    "\n",
    "# Show the results\n",
    "print(\"Monthly average sales:\")\n",
    "monthly_avg_sales.show()\n",
    "\n",
    "print(\"Quarterly average sales:\")\n",
    "quarterly_avg_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date with Maximum Sales: 4245.0\n",
      "Date with Minimum Sales: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Find dates with maximum and minimum total sales\n",
    "max_sales_date = df.groupBy().agg({\"total_cost\": \"max\"}).collect()[0][0]\n",
    "min_sales_date = df.groupBy().agg({\"total_cost\": \"min\"}).collect()[0][0]\n",
    "\n",
    "# Show the results\n",
    "print(\"Date with Maximum Sales:\", max_sales_date)\n",
    "print(\"Date with Minimum Sales:\", min_sales_date)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
